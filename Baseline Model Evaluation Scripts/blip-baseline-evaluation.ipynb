{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11844315,"sourceType":"datasetVersion","datasetId":7441800},{"sourceId":11853176,"sourceType":"datasetVersion","datasetId":7448038}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:18:59.469593Z","iopub.execute_input":"2025-05-17T14:18:59.469854Z","iopub.status.idle":"2025-05-17T14:20:14.597962Z","shell.execute_reply.started":"2025-05-17T14:18:59.469834Z","shell.execute_reply":"2025-05-17T14:20:14.597251Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:07:56.591343Z","iopub.execute_input":"2025-05-18T03:07:56.591581Z","iopub.status.idle":"2025-05-18T03:07:56.595923Z","shell.execute_reply.started":"2025-05-18T03:07:56.591563Z","shell.execute_reply":"2025-05-18T03:07:56.595192Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:09:17.186492Z","iopub.execute_input":"2025-05-18T03:09:17.186853Z","iopub.status.idle":"2025-05-18T03:09:17.191196Z","shell.execute_reply.started":"2025-05-18T03:09:17.186832Z","shell.execute_reply":"2025-05-18T03:09:17.190486Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:09:19.062925Z","iopub.execute_input":"2025-05-18T03:09:19.063402Z","iopub.status.idle":"2025-05-18T03:09:40.381077Z","shell.execute_reply.started":"2025-05-18T03:09:19.063379Z","shell.execute_reply":"2025-05-18T03:09:40.380288Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb5a354402d42f68e4c7deaa1f50326"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45838eceb924342a9fa5ce287f501c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"321843cdd7e7484a91b38f3fbf0db9e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5c2733ff33543c4bbe3257f8d169842"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab80b5cddd0c40478028bbff401c80f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab12766c1f54dc583872b2ce510714b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d591d207ae4c2fa09e314e1b95de9f"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:09:43.491544Z","iopub.execute_input":"2025-05-18T03:09:43.492179Z","iopub.status.idle":"2025-05-18T03:09:43.501839Z","shell.execute_reply.started":"2025-05-18T03:09:43.492147Z","shell.execute_reply":"2025-05-18T03:09:43.501173Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 384,672,572\nTrainable parameters: 384,672,572\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/vqa-inference-dataset/Inference/combined_inference_vqa_single_answer.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:09:46.540230Z","iopub.execute_input":"2025-05-18T03:09:46.540492Z","iopub.status.idle":"2025-05-18T03:09:46.578337Z","shell.execute_reply.started":"2025-05-18T03:09:46.540472Z","shell.execute_reply":"2025-05-18T03:09:46.577714Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        image_path                                           question  \\\n0  64/6412af43.jpg            What is the visible color of the chair?   \n1  64/6412af43.jpg  What is the average height of the back legs in...   \n2  64/64acb3aa.jpg               What kind of food is in the package?   \n3  64/64acb3aa.jpg  How many tortillas in total can the packaging ...   \n4  64/64c3be6d.jpg                        What is the display called?   \n\n      answer  \n0       Gray  \n1      4.125  \n2  Tortillas  \n3        Six  \n4    Monitor  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>64/6412af43.jpg</td>\n      <td>What is the visible color of the chair?</td>\n      <td>Gray</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>64/6412af43.jpg</td>\n      <td>What is the average height of the back legs in...</td>\n      <td>4.125</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>64/64acb3aa.jpg</td>\n      <td>What kind of food is in the package?</td>\n      <td>Tortillas</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64/64acb3aa.jpg</td>\n      <td>How many tortillas in total can the packaging ...</td>\n      <td>Six</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64/64c3be6d.jpg</td>\n      <td>What is the display called?</td>\n      <td>Monitor</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"base_img_dir = \"/kaggle/input/vqa-inference-dataset/Inference/images\"\npredictions = []\nground_truths = []\n\nfor idx, row in tqdm(df.iterrows(), total=len(df)):\n    rel_path = row[\"image_path\"]\n    full_img_path = os.path.join(base_img_dir, rel_path)\n    question = row[\"question\"]\n    true_answer = str(row[\"answer\"]).strip().lower()\n\n    try:\n        image = Image.open(full_img_path).convert(\"RGB\")\n    except Exception as e:\n        print(f\"Failed to load image at {image_path}: {e}\")\n        continue\n\n    inputs = processor(image, question, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=1)\n    pred_answer = processor.decode(out[0], skip_special_tokens=True).strip().lower()\n\n    predictions.append(pred_answer)\n    ground_truths.append(true_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:09:57.978334Z","iopub.execute_input":"2025-05-18T03:09:57.978729Z","iopub.status.idle":"2025-05-18T03:13:38.286729Z","shell.execute_reply.started":"2025-05-18T03:09:57.978707Z","shell.execute_reply":"2025-05-18T03:13:38.286042Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2969/2969 [03:40<00:00, 13.48it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Exact string match accuracy\nacc = accuracy_score(ground_truths, predictions)\nf1 = f1_score(ground_truths, predictions, average='macro')\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1Score (macro): {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:20:14.825614Z","iopub.execute_input":"2025-05-18T03:20:14.825970Z","iopub.status.idle":"2025-05-18T03:20:14.862919Z","shell.execute_reply.started":"2025-05-18T03:20:14.825948Z","shell.execute_reply":"2025-05-18T03:20:14.862312Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.4453\nF1Score (macro): 0.1143\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Other evaluation metrics\n!pip install evaluate bert-score rouge-score --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:20:17.375791Z","iopub.execute_input":"2025-05-18T03:20:17.376054Z","iopub.status.idle":"2025-05-18T03:21:40.313698Z","shell.execute_reply.started":"2025-05-18T03:20:17.376037Z","shell.execute_reply":"2025-05-18T03:21:40.312916Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\n\nbert_score = evaluate.load(\"bertscore\")\nrouge = evaluate.load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:21:52.277031Z","iopub.execute_input":"2025-05-18T03:21:52.277368Z","iopub.status.idle":"2025-05-18T03:21:54.894502Z","shell.execute_reply.started":"2025-05-18T03:21:52.277340Z","shell.execute_reply":"2025-05-18T03:21:54.893695Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b8ca80523f74fa5b050850f1f9def7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e13026a57f4470ba0c9af8bd327969d"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"rouge_results = rouge.compute(predictions=predictions, references=ground_truths)\nprint(rouge_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:22:01.707281Z","iopub.execute_input":"2025-05-18T03:22:01.707936Z","iopub.status.idle":"2025-05-18T03:22:02.733485Z","shell.execute_reply.started":"2025-05-18T03:22:01.707906Z","shell.execute_reply":"2025-05-18T03:22:02.732805Z"}},"outputs":[{"name":"stdout","text":"{'rouge1': 0.4470079712585607, 'rouge2': 0.0, 'rougeL': 0.4474009206242281, 'rougeLsum': 0.44683956438756034}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"bert_results = bert_score.compute(predictions=predictions, references=ground_truths, lang=\"en\")\nprint(f\"BERTScore Precision: {sum(bert_results['precision'])/len(predictions):.4f}\")\nprint(f\"BERTScore Recall: {sum(bert_results['recall'])/len(predictions):.4f}\")\nprint(f\"BERTScore F1: {sum(bert_results['f1'])/len(predictions):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:22:06.789235Z","iopub.execute_input":"2025-05-18T03:22:06.789506Z","iopub.status.idle":"2025-05-18T03:22:20.017263Z","shell.execute_reply.started":"2025-05-18T03:22:06.789488Z","shell.execute_reply":"2025-05-18T03:22:20.016522Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99262cc99ea64fdaad23529e30355aeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2470db90958c4287a78320cda3772a3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"919d02a98371458f92334faa844e24c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11c5cc3962074ee5b7bc19a6bce0f1af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"850c5a681eb748148b88fb5279c1183d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a098e31d5ceb4a67a4841845cf487a75"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"BERTScore Precision: 0.9782\nBERTScore Recall: 0.9634\nBERTScore F1: 0.9702\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!git clone https://github.com/neulab/BARTScore.git\n\n%cd BARTScore\n!pip install -r requirements.txt --quiet\n!pip install transformers==4.11.3 --quiet \n\nimport sys\nsys.path.append('/kaggle/working/BARTScore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:22:25.207298Z","iopub.execute_input":"2025-05-18T03:22:25.207605Z","iopub.status.idle":"2025-05-18T03:22:51.218605Z","shell.execute_reply.started":"2025-05-18T03:22:25.207583Z","shell.execute_reply":"2025-05-18T03:22:51.217876Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'BARTScore'...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"remote: Enumerating objects: 220, done.\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 220 (delta 18), reused 14 (delta 14), pack-reused 194 (from 1)\u001b[K\nReceiving objects: 100% (220/220), 101.98 MiB | 11.27 MiB/s, done.\nResolving deltas: 100% (47/47), done.\nUpdating files: 100% (192/192), done.\n/kaggle/working/BARTScore\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: Could not find a version that satisfies the requirement BLEURT==0.0.2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for BLEURT==0.0.2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from bart_score import BARTScorer\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbart_scorer = BARTScorer(device=device, checkpoint='facebook/bart-large-cnn')\nscores = bart_scorer.score(predictions, ground_truths, batch_size=8)\nprint(f\"Avg BARTScore: {sum(scores)/len(scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:22:55.614814Z","iopub.execute_input":"2025-05-18T03:22:55.615566Z","iopub.status.idle":"2025-05-18T03:23:26.191912Z","shell.execute_reply.started":"2025-05-18T03:22:55.615538Z","shell.execute_reply":"2025-05-18T03:23:26.191062Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a733ffcd3df54f6bbd991445ddb57498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1a4ada14f645cc8207386cc04ad614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc931b1ed7241a6a43a01456552aa54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdbb896233f74ece94b721aa5c61c533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b1b7a066006408da7c0464c53d6f417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df52fd6d5714baebf7bee733caff948"}},"metadata":{}},{"name":"stdout","text":"Avg BARTScore: -4.6868\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}