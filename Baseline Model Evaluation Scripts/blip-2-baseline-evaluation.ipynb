{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11844315,"sourceType":"datasetVersion","datasetId":7441800},{"sourceId":11853176,"sourceType":"datasetVersion","datasetId":7448038}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:20:54.396101Z","iopub.execute_input":"2025-05-17T15:20:54.396555Z","iopub.status.idle":"2025-05-17T15:22:22.061219Z","shell.execute_reply.started":"2025-05-17T15:20:54.396529Z","shell.execute_reply":"2025-05-17T15:22:22.060383Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom nltk.corpus import stopwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:46:08.780270Z","iopub.execute_input":"2025-05-18T02:46:08.780528Z","iopub.status.idle":"2025-05-18T02:46:50.235122Z","shell.execute_reply.started":"2025-05-18T02:46:08.780506Z","shell.execute_reply":"2025-05-18T02:46:50.234450Z"}},"outputs":[{"name":"stderr","text":"2025-05-18 02:46:28.615579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747536389.095845      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747536389.226088      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:46:52.460396Z","iopub.execute_input":"2025-05-18T02:46:52.460969Z","iopub.status.idle":"2025-05-18T02:46:52.465582Z","shell.execute_reply.started":"2025-05-18T02:46:52.460943Z","shell.execute_reply":"2025-05-18T02:46:52.464786Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:46:54.293530Z","iopub.execute_input":"2025-05-18T02:46:54.293868Z","iopub.status.idle":"2025-05-18T02:48:44.714836Z","shell.execute_reply.started":"2025-05-18T02:46:54.293813Z","shell.execute_reply":"2025-05-18T02:48:44.714216Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa4c0c0367b4d24b13ca34cc4316dd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62956a009de24ed0be1b670b08d63d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcf52b46744848bf841c2d9c736e2432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764fba58c33041ec9a533aa7ad0ee2b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec89043da4df4fb1b3352936d4662342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ff491b9aad4c0d862c679cc299adc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa5f2459d1943e1a536413378c68f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a69f99efe3e4208bcad2127f0b500c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43f47e3c23f743c4877cce2b05b782e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548f4c73e76d4a72b288fef4c8cad181"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976590e5ffd74bf89547d128a6c42bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/5.81G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b99d57fced254bbab3058c5eefc28fc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eedad4bbe81c4ad88ee920df1a646922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e77f367dd0f42d69f6b6f77430e899e"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:48:48.127455Z","iopub.execute_input":"2025-05-18T02:48:48.127730Z","iopub.status.idle":"2025-05-18T02:48:48.140973Z","shell.execute_reply.started":"2025-05-18T02:48:48.127709Z","shell.execute_reply":"2025-05-18T02:48:48.140252Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 3,942,446,592\nTrainable parameters: 3,942,446,592\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/vqa-inference-dataset/Inference/combined_inference_vqa_single_answer.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:48:51.025818Z","iopub.execute_input":"2025-05-18T02:48:51.026443Z","iopub.status.idle":"2025-05-18T02:48:51.097971Z","shell.execute_reply.started":"2025-05-18T02:48:51.026421Z","shell.execute_reply":"2025-05-18T02:48:51.097364Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        image_path                                           question  \\\n0  64/6412af43.jpg            What is the visible color of the chair?   \n1  64/6412af43.jpg  What is the average height of the back legs in...   \n2  64/64acb3aa.jpg               What kind of food is in the package?   \n3  64/64acb3aa.jpg  How many tortillas in total can the packaging ...   \n4  64/64c3be6d.jpg                        What is the display called?   \n\n      answer  \n0       Gray  \n1      4.125  \n2  Tortillas  \n3        Six  \n4    Monitor  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>64/6412af43.jpg</td>\n      <td>What is the visible color of the chair?</td>\n      <td>Gray</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>64/6412af43.jpg</td>\n      <td>What is the average height of the back legs in...</td>\n      <td>4.125</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>64/64acb3aa.jpg</td>\n      <td>What kind of food is in the package?</td>\n      <td>Tortillas</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>64/64acb3aa.jpg</td>\n      <td>How many tortillas in total can the packaging ...</td>\n      <td>Six</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64/64c3be6d.jpg</td>\n      <td>What is the display called?</td>\n      <td>Monitor</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"base_img_dir = \"/kaggle/input/vqa-inference-dataset/Inference/images\"\npredicted_words = []\npredicted_answers = []\nground_truths = []\n\nfor idx, row in tqdm(df.iterrows(), total=len(df)):\n    image_path = os.path.join(base_img_dir, row['image_path'])\n    question = row['question']\n    true_answer = str(row['answer']).strip().lower()\n\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n    except Exception as e:\n        print(f\"Failed to load image at {image_path}: {e}\")\n        continue\n\n    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=3)\n    pred_answer = processor.decode(output[0], skip_special_tokens=True).strip().lower()\n    predicted_answers.append(pred_answer)\n    \n    words = [w for w in pred_answer.split() if w.lower() not in stopwords.words('english')]\n    pred_word = words[0] if words else 'unknown'\n    \n    predicted_words.append(pred_word)\n    ground_truths.append(true_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T02:48:54.727464Z","iopub.execute_input":"2025-05-18T02:48:54.727959Z","iopub.status.idle":"2025-05-18T03:00:13.406708Z","shell.execute_reply.started":"2025-05-18T02:48:54.727934Z","shell.execute_reply":"2025-05-18T03:00:13.406134Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2969/2969 [11:18<00:00,  4.37it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Exact string match accuracy\nacc = accuracy_score(ground_truths, predicted_words)\nf1 = f1_score(ground_truths, predicted_words, average='macro')\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"F1 Score (macro): {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:00:56.184366Z","iopub.execute_input":"2025-05-18T03:00:56.184663Z","iopub.status.idle":"2025-05-18T03:00:56.256018Z","shell.execute_reply.started":"2025-05-18T03:00:56.184639Z","shell.execute_reply":"2025-05-18T03:00:56.255215Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.1701\nF1 Score (macro): 0.0376\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Other evaluation metrics\n!pip install evaluate bert-score rouge-score --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:01:00.189069Z","iopub.execute_input":"2025-05-18T03:01:00.189760Z","iopub.status.idle":"2025-05-18T03:02:37.224847Z","shell.execute_reply.started":"2025-05-18T03:01:00.189734Z","shell.execute_reply":"2025-05-18T03:02:37.223886Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\n\nbert_score = evaluate.load(\"bertscore\")\nrouge = evaluate.load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:02:59.114278Z","iopub.execute_input":"2025-05-18T03:02:59.115123Z","iopub.status.idle":"2025-05-18T03:03:01.606495Z","shell.execute_reply.started":"2025-05-18T03:02:59.115092Z","shell.execute_reply":"2025-05-18T03:03:01.605849Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986c8a1d313f43f7a1f4247bbb8685a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a5563a3630140b5b5eec28eaf2f474c"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"rouge_results = rouge.compute(predictions=predicted_answers, references=ground_truths)\nprint(rouge_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:03:08.748909Z","iopub.execute_input":"2025-05-18T03:03:08.749963Z","iopub.status.idle":"2025-05-18T03:03:09.694167Z","shell.execute_reply.started":"2025-05-18T03:03:08.749937Z","shell.execute_reply":"2025-05-18T03:03:09.693391Z"}},"outputs":[{"name":"stdout","text":"{'rouge1': 0.1280172897720899, 'rouge2': 0.0, 'rougeL': 0.12745593353542206, 'rougeLsum': 0.1283260357022572}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"bert_results = bert_score.compute(predictions=predicted_answers, references=ground_truths, lang=\"en\")\nprint(f\"BERTScore Precision: {sum(bert_results['precision'])/len(predicted_answers):.4f}\")\nprint(f\"BERTScore Recall: {sum(bert_results['recall'])/len(predicted_answers):.4f}\")\nprint(f\"BERTScore F1: {sum(bert_results['f1'])/len(predicted_answers):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:03:34.413099Z","iopub.execute_input":"2025-05-18T03:03:34.413390Z","iopub.status.idle":"2025-05-18T03:03:49.403021Z","shell.execute_reply.started":"2025-05-18T03:03:34.413371Z","shell.execute_reply":"2025-05-18T03:03:49.402348Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1a4fa201914e9fae4fb6819d66f1bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5044381cbc46f69d607bc001b2fb51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca73c1a85814c7b9a5fa9abba945345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"637b370dca3f452fb5f507747a245990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e5a5ad69dd42469617aab771cf440c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5da8d637b874394af107601f5c103ce"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"BERTScore Precision: 0.8854\nBERTScore Recall: 0.9161\nBERTScore F1: 0.8994\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!git clone https://github.com/neulab/BARTScore.git\n\n%cd BARTScore\n!pip install -r requirements.txt --quiet\n!pip install transformers==4.11.3 --quiet \nimport sys\nsys.path.append('/kaggle/working/BARTScore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:03:56.561062Z","iopub.execute_input":"2025-05-18T03:03:56.561349Z","iopub.status.idle":"2025-05-18T03:04:16.889153Z","shell.execute_reply.started":"2025-05-18T03:03:56.561329Z","shell.execute_reply":"2025-05-18T03:04:16.888095Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'BARTScore'...\nremote: Enumerating objects: 220, done.\u001b[K\nremote: Counting objects: 100% (26/26), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 220 (delta 18), reused 14 (delta 14), pack-reused 194 (from 1)\u001b[K\nReceiving objects: 100% (220/220), 101.98 MiB | 22.24 MiB/s, done.\nResolving deltas: 100% (47/47), done.\nUpdating files: 100% (192/192), done.\n/kaggle/working/BARTScore\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: Could not find a version that satisfies the requirement BLEURT==0.0.2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for BLEURT==0.0.2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from bart_score import BARTScorer\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbart_scorer = BARTScorer(device=device, checkpoint='facebook/bart-large-cnn')\nscores = bart_scorer.score(predicted_answers, ground_truths, batch_size=8)\nprint(f\"Avg BARTScore: {sum(scores)/len(scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:04:20.900647Z","iopub.execute_input":"2025-05-18T03:04:20.900997Z","iopub.status.idle":"2025-05-18T03:04:38.966660Z","shell.execute_reply.started":"2025-05-18T03:04:20.900969Z","shell.execute_reply":"2025-05-18T03:04:38.966014Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1995b39cdad74cd5b93130294cd7aa59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6703d68b354ffe942335ff37a12c46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10f3750e8bd425ea6f0feec3c90caf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cda2ca932664ec2b4a1c012fc90ef07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9242f63fd69c4e3fba89c51bce03ccd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e550b9bcf5c34c539477fbe83c88d434"}},"metadata":{}},{"name":"stdout","text":"Avg BARTScore: -6.5398\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}